# Teacher, Data Analyst/Scientist & Product Data Manager
    - See blue link to the portfolio toward the end of the resume
## Professional Profile
As a seasoned Data Scientist with extensive experience extracting valuable insights and knowledge from complex datasets, I specialise in employing advanced visualisation and statistical analysis techniques. My expertise lies in identifying and interpreting interrelationships between various features within datasets, contributing significantly to understanding and predicting target variables and features. Proficient in not only the technical aspects of data science but also in navigating the legal and ethical constraints associated with handling sensitive data; I ensure that all practices adhere to the highest standards of data ethics and compliance. This blend of technical understanding and ethical awareness positions me as a versatile and responsible professional in data science.

## Technical Skills: Python, PowerBI, Excel, TABLEAU

## Education
- #### SCQF LEVEL 9, Data Science with Python | The Robert Gordon University Aberdeen, UK (December 2023)
- #### M.Sc., HRM | The Robert Gordon University Aberdeen, UK (November 2009)
- #### MPSM., Economics, Policy & Project Management  | Ghana Institute of Management and Public Administration, Accra, Ghana (December 2006)
- #### B.Sc[ED] Hons., Economics | The Lagos State University Lagos, Nigeria (September 2002)

## Work Experience

#### Product Data Manager @ ABM Multisector Ltd (Sept 2010 - Present)
A results-oriented Product Manager with a sincere dedication to crafting user-focused products that bolster business growth. Demonstrates a proven track record in translating market insights into effective product strategies and steering agile development teams to deliver premium products punctually and within budget.

**Experience/Skills**
A results-driven Product Manager with a commitment to user-centric product development, leveraging data analytics and agile methodologies to drive business growth.
- Conduct comprehensive market research and competitor analysis, resulting in a 10% increase in market share. Using Helium 10 and Keepa data analytics tools to inform my judgement.
- Implement agile methodologies, boosting product development efficiency by 25%.
- Collaborate with stakeholders to prioritize features based on customer needs, enhancing customer retention by 10%.
- Manage product budgets and resources effectively, maximizing ROI.
- Utilize data analytics and user feedback to refine product decisions continuously.

These strategies led to successful product launches, improved usability by 15%, and fostered enduring client relationships, contributing to a 20% rise in product adoption and mitigating risks throughout the product lifecycle.
**Skills:** Data Analysis, Web Design and Development (jobruns.com) · Project Planning · Human Resources Information Systems (HRIS) · Microsoft Power BI · Agile Methodologies · Leadership · Business Strategy · Python (Programming Language) · Data Analysis


#### Teacher, Economics, Business Management, Information Technology @ Aberdeen City Council (January 2015 - Present)
**Experience/Skills**
- Technological Fluency Advancement: Demonstrates a strong track record in enhancing students’ technological fluency, which is crucial for navigating a digitalised future.
- Engaging Curriculum Development: Skilled in crafting captivating curricula that inspire dynamic learning experiences and foster student interest and participation.
- Utilisation of Modern Teaching Tools: Utilises state-of-the-art teaching tools and digital media to maximise student engagement and comprehension, ensuring effective learning outcomes.
- Comprehensive Student Progress Monitoring: Diligently tracks student progress and sets ambitious targets, providing accurate predictions for examination outcomes and guiding students toward academic success.
- Holistic Student Support and Development: A student mentor effectively communicates progress and improvement areas to students and families. Leads extracurricular activities promoting well-rounded growth while maintaining positive educational standards.

#### Business Data Analyst @ Centre for Energy Research and Development, Nigeria (January 2002 - August 2008)
Engineered, designed, and maintained advanced data warehouses and BI solutions; Transformed diverse data sources into organised formats using BI tools; Produced valuable insights and comprehensive reports for informed decisions. 

**Experience/Skills**
- Advanced Data Infrastructure Management: Spearheaded the engineering, design, and maintenance of sophisticated data warehouses and BI solutions, ensuring optimal performance and functionality.
- Data Transformation Expertise: Proficiently converted diverse data sources into structured formats using cutting-edge BI tools, facilitating streamlined analysis and decision-making processes.
- Insightful Reporting and Decision Support: I produced comprehensive reports and valuable insights, empowering stakeholders with the information needed to make informed decisions and drive business growth.
- Efficiency Enhancement: Optimised I optimised warehouse architecture and implemented advanced BI tools, resulting in a notable 32% improvement in data quality, a 21% reduction in time b, and a 48% reduction in costs	
- Data Security Assurance: We implemented and ensured strong data security measures, safeguarding sensitive information against unauthorised access or breaches.
Seamless Data Modeling: Designed models for seamless querying and analysis, enhancing the efficiency and accuracy of data-driven insights and decision-making processes.

# Projects Portfolio
### Programming Languages & Software Skills
- Proficient in SQL, Python, and similar languages; these technical skills form the backbone of my expertise. In my previous roles, I have utilised SQL for database management and Python for data analysis and automation tasks. For example, I developed a Python-based tool to automate data collection in a recent project, significantly improving efficiency.

### [Data Modelling Project](https://github.com/aadegoke74/Data-Modelling-Project)
#### - Explanation of why delay, date or port should not be used as predictors
Here are more technical reasons for the non-inclusion of variables and the selection of predictors:
    
- **'Delay':** This variable may not be a reliable predictor due to its susceptibility to confounding factors or external influences not captured in the dataset. Various unmeasured variables could influence' Delay',             making it a poor predictor of the outcome. Its inclusion without proper consideration could lead to misleading conclusions or spurious correlations.
   
- **'Date':** The 'Date' variable might introduce seasonality effects, which could complicate the modelling process and require advanced techniques to account for temporal patterns adequately. Without proper handling,           the 'Date' variable may not contribute meaningfully to the prediction task and could introduce noise or bias into the model.
   
- **'Port':** While 'Port' may provide contextual information, it may not directly influence the outcome and could lead to overfitting or biased predictions if used as a predictor without careful consideration. The                 geographical specifics represented by 'Port' may not be inherently linked to the outcome variable, making it a less informative predictor than other variables with stronger causal relationships.

By excluding these variables or treating them carefully in the modelling process, I can mitigate the risk of introducing spurious correlations and ensure that the selected predictors contribute meaningfully to the model's predictive performance.

#### - Justify the choice of the variables that are used as predictors
Selecting suitable predictors is pivotal for a model's predictive accuracy and intelligibility. Such predictors warrant theoretical or empirical grounding to affirm their relation to the dependent variable. For instance, in maritime logistics, 'TEU' and 'time' are cogent predictors for efficiency, directly quantifying cargo size and processing duration. Similarly, 'gear' could be a robust predictor given its direct operational impact. This aligns with Sharma and Ranjan's (2023) perspective on predictive modelling in intelligent systems.

In contrast, indicators like 'delay' or confounders like 'date', which may obscure underlying seasonal influences, demand a cautious approach. While providing locational context, the variable' port' could introduce bias if not correctly adjusted for operational variability. Hao and Ho's (2019) exposition on machine learning tools like sci-kit-learn further elucidates the necessity of selecting predictors that enhance a model's utility and interpretability, advocating for a rigorous and actionable methodology in analytics.

### [Data Transformation Project](https://github.com/aadegoke74/Data-Transformation-Project/blob/main/Section%204%20Data%20Transformation%20Project.ipynb)
- Proficient in executing data transformation projects, adept at manipulating and converting data across various formats and structures to meet project requirements. Experienced in utilizing advanced data transformation techniques and tools to cleanse, normalize, and integrate data from diverse sources. Skilled in developing and implementing efficient data transformation pipelines, ensuring transformed data accuracy, consistency, and reliability.
- Familiar with industry-standard data transformation methodologies and best practices, capable of optimizing data workflows and automating transformation processes for enhanced productivity and efficiency. Strong analytical and problem-solving abilities combined with meticulous attention to detail in managing data transformation projects effectively.

      (a). In the ports2 dataset, create a new data column called unloadrate. using the formula: unloadrate = TEU / time
      (b). Plot unloadrate against TEU on a scatterplot and colour code the points by delay category.

The boxplots illustrate the variability in ships' time at the port, segmented by year and month. The annual comparison between 2021 and 2022 shows overlapping interquartile ranges (IQRs), suggesting that the central tendencies of port times may not have significantly shifted year-on-year. Notably, an outlier in 2021 marks an exceptional duration at the port, potentially highlighting an irregular delay.

The monthly distribution across the year indicates a palpable seasonality effect on port times. The IQRs and medians ebb and flow, possibly reflecting operational influences or environmental factors inherent to British seasons. For instance, increased port times in winter could be attributed to adverse weather conditions, while the summer months might experience more efficiency.

No outliers are present in 2022, which could signify a more consistent operation at ports compared to 2021. Nonetheless, outliers and the breadth of IQRs in the monthly data across both years suggest that external or operational factors periodically impact port times. Interpreting these findings cautiously is imperative, as they offer a descriptive snapshot rather than a definitive analysis. A deeper dive into the data is recommended to unravel the causative factors behind these observed temporal patterns.
  
### [Bivariate Data Exploration Project](https://github.com/aadegoke74/Bivariate-Data-Exploration-Project/blob/main/Bivariate%20Data%20Exploration%20Project.ipynb)
- Statistics and distribution of the numerical time against one of the categorical variables
- An initial exploratory analysis is necessary to analyse how categorical variables affect the numerical "time" variable, which represents ship processing time. This includes computing average times per category and performing statistical tests to determine if variations in means are significant. Key categorical variables such as "weather," "onSchedule," "labour," "origin," "delay," and "port" are considered for their potential impact on processing time.
- Summary statistics and visual tools like boxplots will be used to discern disparities in processing time across different categories, indicating if deeper statistical evaluation is justified. The process involves calculating the mean, median, and standard deviation of "time" for each categorical variable and creating boxplots to observe distributions, outliers, and notable differences.

- Visualisations: (See blue link) The provided summary statistics suggest that 'weather' impacts port time, with 'disruptive' conditions leading to a slightly higher mean(265) time than 'normal' conditions (234). The 'onSchedule' variable shows a more pronounced difference, where not being on schedule is associated with an increased mean port time. Under 'labour', 'striking' has a higher mean time than 'working', consistent with the expectation that strikes affect operational efficiency. For 'origin', 'Intercontinental' ships have the highest mean time, possibly due to longer journey times or more complex cargo handling.

 - There's a distinct difference in mean time between ships that experienced 'delays' and those that did not, with delays increasing the average time spent at port. The 'port' data, focused solely on 'Abermouth', presents an average time within a broad range, indicating varied port time, even within a single location. These figures highlight the influence of external and operational factors on port efficiency, with the potential for deeper investigation into the causes of these variations.

Next, I will be considering the statistics and distribution of the numerical time against one of the categorical variables that significantly affect the ship processing time.

 - Boxplot & Correlation Matrix HeatMap Visualisations: (See blue link) Correlation of the numerical time against the other numerical variables
 - Cross-tabulation Visualisations: (See blue link) of category onSchedule against the category origin.

### [Data Exploration Project - Statistics of individual variables](https://github.com/aadegoke74/Data-Exploration-Project/blob/main/Data%20Exploration%20Tasks%20-%20Statistics%20of%20individual%20variables.ipynb) 
- Skilled in performing data exploration tasks, including statistical analysis of individual variables. Proficient in extracting insights and trends from data through thoroughly examining variable distributions and descriptive statistics.
- I calculate relevant summary statistics and create visualisations to summarise and comment upon the data for the three numerical variables (time, TEU, and wind).

a) Time: a continuous numerical variable that likely represents a period or timestamp. Summary statistics and visualisations for time data can provide insights into patterns and trends over time.
Summary Statistics: 
 - Mean (average time)
 - Median (middle time)
 - Standard Deviation (spread of time values)
 - Minimum and Maximum (start and end times)
 - Quartiles (25th, 50th, 75th percentiles)

Visualisations: (See blue link)
 - Time Series Plot: I create a line chart showing how the variable changes over time. This can reveal trends, seasonality, and outliers.
 - Histogram or Density Plot: Create a histogram or density plot to show the distribution of time values. This can help identify any patterns in the time data.

b) TEU (Twenty-foot Equivalent Unit): a numerical variable representing cargo container capacity.
Summary Statistics:
 - Mean (average TEU) Median (middle TEU) Standard Deviation (spread of TEU values) Minimum and Maximum (minimum and maximum TEU) Quartiles (25th, 50th, 75th percentiles) Visualizations:
 - Histogram: Create a histogram to visualize the distribution of TEU values. Helps identify common capacity values.
 - Box Plot: A box plot can show the median, quartiles, and any outliers in the TEU data, giving insights into the spread of capacity values.

c) Wind: a numerical variable that measures wind speed.
Summary Statistics:
 - Mean (average wind speed) Median (middle wind speed) Standard Deviation (spread of wind speed values) Minimum and Maximum (minimum and maximum wind speed) Quartiles (25th, 50th, 75th percentiles) Visualizations:
 - Histogram: Create a histogram to visualize the distribution of wind speed values. This can help identify typical wind speed ranges.
 - Box Plot: A box plot can show the median, quartiles, and any outliers in the wind speed data, giving insights into the spread of wind speed values.

Comments:
 - For time data, I observe time patterns/trends.
 - TEU data helps understand the capacity of cargo containers.
 - Wind data provide insights into the wind speed conditions at the ports.

### [Data Cleaning Project](https://github.com/aadegoke74/Data-Cleaning-Project/blob/main/Data%20Cleaning%20Tasks.ipynb) 
- a) To identify and replace any cells containing missing data in the DataFrame, I will follow these steps:
    1. Load the data from the CSV file into a pandas DataFrame.
    2. Examine the data to identify which columns are numerical or categorical.
    3. For numerical columns: Calculate the mean of the column. Replace missing values with the mean of that column.
    4. For categorical columns: Determine the mode (most common value) of the column. Replace missing values with the mode of that column.

Deliberately renaming the DataFrame to ports_replacemissing is a good practice in data analysis, as it provides clarity, prevents potential conflicts, and sets the stage for specific data processing tasks.

### Data cleansing and wrangling
 - Various methods have been used historically for missing values imputation. Dadi, K et al. (2021) used mean for imputation as it maintains the column's overall distribution and it is widely used. Therefore, their mean shall replace missing values for Gear and Wind.

 - b) To identify and remove any duplicate rows from the DataFrame, I will:
   1. Use the duplicated() method to identify any duplicate rows, marking them as True if they are duplicates.
   2. Use the drop_duplicates() method to remove the duplicate rows.
   3. I will then display the number of duplicate rows identified and removed.

The dataset contains six duplicate rows. I will now remove these duplicates and confirm their removal.
Damodaram (2022) and Steorts (2023) emphasise the importance of removing duplicates in data management and analysis, highlighting its role in maintaining data integrity and quality.
 - The code below identifies and displays the number of duplicate rows in the 'ports' Frame. (See blue link)

 - c) Convert the rain column from text to numerical format.
    1. I will strip the "mm" from the values and convert the column to a float type.
    2. The 'rain' column's "mm" values have been stripped, converting the data to a numerical type. It is crucial to format data appropriately for precise and efficient processing in data analysis. Take the 'rain' column as an example: changing values from a string format like "3.1mm" to a numerical one such as 3.1 is essential for quantitative analysis.
    3. Using numerical data allows for various statistical calculations, such as computing averages, examining correlations, and performing regression analyses, which are complex with string data.

 - d) Standardise the labour column to have consistent formatting.
    1. I intend to unify the 'labour' column by substituting '0' with 'striking' and '1' with 'working', ensuring consistency throughout the column.
    2. I have made the 'labour' column consistent by replacing '0' with 'striking' and '1' with 'working'. This standardisation of categorical data, demonstrated by the transformation of '0' and '1' to 'striking'             and 'working', enhances clarity and uniformity in the dataset. It not only aids in category-based analysis but also improves the readability of the data.
    3. This process of data cleaning and standardisation is crucial, as emphasised by Steorts (2023) in "A Primer on the Data Cleaning Pipeline" and Zacks, Kenett, and Gedeck (2023) in "Modern Statistics: A                     Computer-Based Approach with Python Solutions." These authors underline the importance of thorough data preparation for achieving accurate and insightful analysis, highlighting the fundamental role of              data preprocessing in statistics.


## References
- Available on Request
